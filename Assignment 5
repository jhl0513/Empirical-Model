import pandas as pd
import re
import gensim
from gensim import corpora
from gensim.models.ldamodel import LdaModel
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")

# ========== STEP 1: Load Yelp Review Data ==========
df = pd.read_csv("yelp_homework.csv")
texts = df["review"].astype(str)

# ========== STEP 2: Load Stop Words ==========
stop_df = pd.read_csv("yelp.stop", names=["words"])
stop_words = stop_df["words"].str.strip().tolist()

# ========== STEP 3: Clean and Tokenize ==========
def clean_text(text, stop_words):
    text = re.sub(r"[^\w\s]", "", text.lower())
    return ' '.join([word for word in text.split() if word not in stop_words and len(word) > 2])

cleaned_texts = texts.apply(lambda x: clean_text(x, stop_words))
tokenized_texts = [doc.split() for doc in cleaned_texts if doc.strip() != ""]

# ========== STEP 4: Loop over vocab sizes and topic counts ==========
vocab_sizes = [200, 800]
topic_counts = [5, 15, 25, 35, 45]
all_results = []

for top_n in vocab_sizes:
    print(f"\n=========================")
    print(f"Running with TOP {top_n} words")
    print(f"=========================\n")

    # Create dictionary
    dictionary = corpora.Dictionary(tokenized_texts)
    dictionary.filter_extremes(no_below=5, no_above=0.5)

    # Limit dictionary to top N words by total frequency
    dictionary.filter_tokens(bad_ids=[
        tokenid for tokenid, freq in sorted(dictionary.cfs.items(), key=lambda x: -x[1])[top_n:]
    ])
    dictionary.compactify()

    # Create corpus
    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]
    train_corpus, test_corpus = train_test_split(corpus, test_size=0.2, random_state=42)

    for n_topics in topic_counts:
        print(f"\nTraining LDA with {n_topics} topics (vocab: {top_n})...")
        lda = LdaModel(corpus=train_corpus,
                       id2word=dictionary,
                       num_topics=n_topics,
                       iterations=450,
                       random_state=275,
                       alpha='symmetric',
                       eta='auto')

        perplexity = lda.log_perplexity(test_corpus)
        all_results.append({
            "topics": n_topics,
            "vocab_size": top_n,
            "perplexity": perplexity,
            "model": lda
        })

        print(f"Log Perplexity: {perplexity:.4f}")
        print(f"Top words per topic:")
        for i, topic in lda.show_topics(num_topics=n_topics, num_words=7, formatted=False):
            top_words = [word for word, _ in topic]
            print(f"Topic {i+1}: {' | '.join(top_words)}")

# ========== STEP 5: Print Summary ==========
print("\n===================")
print("Log Perplexity Summary")
print("===================\n")
for result in all_results:
    print(f"Vocab: {result['vocab_size']} | Topics: {result['topics']} â†’ Log Perplexity: {result['perplexity']:.4f}")

# ========== STEP 6: Top Words ===============

# List of (word, frequency) sorted from most to least frequent
top_words_by_freq = sorted([(dictionary[id], freq) for id, freq in dictionary.cfs.items()],
                           key=lambda x: -x[1])

for word, freq in top_words_by_freq[:50]:
    print(f"{word}: {freq}")

# =========== Part 3 ==================

# STEP 4: Dictionary & Corpus (top 800 words)
dictionary = corpora.Dictionary(tokenized_texts)
dictionary.filter_extremes(no_below=5, no_above=0.5)
dictionary.filter_tokens(bad_ids=[
    tokenid for tokenid, freq in sorted(dictionary.cfs.items(), key=lambda x: -x[1])[800:]
])
dictionary.compactify()
corpus = [dictionary.doc2bow(text) for text in tokenized_texts]

# STEP 5: Re-run LDA with different alpha values
alpha_values = [0.001, 0.1, 1.0, 10]
topic_distributions = {}

for alpha in alpha_values:
    print(f"\nRunning LDA with alpha={alpha} ...")
    lda = LdaModel(corpus=corpus,
                   id2word=dictionary,
                   num_topics=25,
                   iterations=450,
                   alpha=alpha,
                   eta='auto',
                   random_state=275)

    # Get topic distributions for the first 9 documents
    doc_topics = []
    for i in range(9):
        topic_probs = lda.get_document_topics(corpus[i], minimum_probability=0)
        doc_topics.append([prob for _, prob in sorted(topic_probs, key=lambda x: x[0])])

    topic_distributions[alpha] = pd.DataFrame(doc_topics)

# Save for plotting
for alpha, df_alpha in topic_distributions.items():
    df_alpha.columns = [f"Topic {i+1}" for i in range(25)]
    df_alpha.index = [f"Doc {i+1}" for i in range(9)]
    df_alpha.to_csv(f"lda_doc_topics_alpha_{alpha}.csv")
